This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-04T10:18:42.665Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.env.example
.github/workflows/main.yml
.gitignore
.streamlit/config.toml
app.py
data/cache/address_cache.json
dir.txt
README.md
requirements.txt
setup_mongodb.py
static/styles.css
utils/__init__.py
utils/address_standardizer.py
utils/auth.py
utils/data_processor.py
utils/database.py

================================================================
Repository Files
================================================================

================
File: .env.example
================
# .env.example
MONGODB_URI=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/<database>?retryWrites=true&w=majority
JWT_SECRET_KEY=your_generated_jwt_secret_key_here
NOMINATIM_USER_AGENT=sothebys_international_realty_nyc

================
File: .github/workflows/main.yml
================
name: Deploy to Streamlit Cloud

on:
  push:
    branches: [ main ]
  pull_request:
    branc

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
ENV/

# Environment variables
.env
.env.local
.env.*.local

# IDE
.idea/
.vscode/
*.swp
*.swo

# Project specific
data/inputs/*
data/outputs/*
!data/inputs/.gitkeep
!data/outputs/.gitkeep

# Streamlit secrets
.streamlit/secrets.toml

# System files
.DS_Store
Thumbs.db

================
File: .streamlit/config.toml
================
[theme]
primaryColor = "#FFFFFF"
backgroundColor = "#002A5C"
secondaryBackgroundColor = "#001A3C"
textColor = "#FFFFFF"
font = "sans serif"

[server]
enableCORS = true  # Changed to true to resolve the warning
enableXsrfProtection = true

[browser]
gatherUsageStats = false

================
File: app.py
================
# app.py
import streamlit as st
import pandas as pd
import os
from datetime import datetime
from utils.data_processor import DataProcessor
from utils.auth import AuthHandler
import base64

def load_css(css_file):
    """Load CSS file"""
    try:
        with open(css_file) as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    except Exception as e:
        st.error(f"Error loading CSS file: {str(e)}")

# Initialize handlers
auth_handler = AuthHandler()

# Configure page
st.set_page_config(
    page_title="Sotheby's Address Validator",
    page_icon="./assets/sothebys-favicon.ico",
    layout="wide"
)

# Load CSS
load_css('static/styles.css')

# Helper functions
def safe_read_excel(file):
    """Safely read Excel file, handling empty files and other errors."""
    try:
        return pd.read_excel(file)
    except (pd.errors.EmptyDataError, ValueError):
        st.error("The uploaded file appears to be empty or invalid.")
        return None
    except Exception as e:
        st.error(f"Error reading file: {str(e)}")
        return None

def get_readable_timestamp():
    """Get current timestamp in readable format."""
    return datetime.now().strftime("%B %d, %Y at %I:%M %p")

def get_file_timestamp():
    """Get timestamp for file naming."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def load_image(image_path):
    try:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode()
    except Exception:
        return None

def display_logo(context='main'):
    """Display logo with context-specific styling"""
    logo_data = load_image('./assets/sothebys-logo.png')
    if logo_data:
        container_class = 'logo-container-login' if context == 'login' else 'logo-container-main'
        st.markdown(f"""
            <div class="{container_class}">
                <img src="data:image/png;base64,{logo_data}" />
            </div>
        """, unsafe_allow_html=True)

# Configure valid property classes
VALID_PROPERTY_CLASSES = ["CD", "B9", "B2", "B3", "CO", "C0" "B1", "C1", "A9", "C2"]
data_processor = DataProcessor(VALID_PROPERTY_CLASSES)

# Authentication check
if 'user_token' not in st.session_state:
    st.session_state.user_token = None

# Login Section Update
if not st.session_state.user_token:
    display_logo('login')
    st.markdown('<h1 class="standard-text">Login</h1>', unsafe_allow_html=True)
    
    # Center the login form
    col1, col2, col3 = st.columns([1,2,1])
    with col2:
        with st.form("login_form"):
            st.markdown("""
                <div class="login-container">
                    <h3 style='text-align: center; color: #002A5C; margin-bottom: 20px; font-weight: 500;'>
                        Use your @sothebys.realty email to login
                    </h3>
            """, unsafe_allow_html=True)
            
            email = st.text_input("Email", placeholder="example@sothebys.realty")
            password = st.text_input("Password", type="password")
            submitted = st.form_submit_button("Login")
            
            if submitted:
                if not email or not password:
                    st.error("Please enter both email and password")
                else:
                    if auth_handler.verify_email_domain(email):
                        st.info(f"Attempting login with email: {email}")
                        token = auth_handler.login(email, password)
                        if token:
                            st.session_state.user_token = token
                            st.rerun()
                        else:
                            st.error(f"Invalid credentials for {email}")
                    else:
                        st.error("Please use your Sotheby's International Realty email (@sothebys.realty)")
            st.markdown("</div>", unsafe_allow_html=True)
    st.stop()

# Main application (only shown to authenticated users)
user = auth_handler.verify_token(st.session_state.user_token)
if not user:
    st.session_state.user_token = None
    st.rerun()

display_logo('main')

# Sidebar with user info and logout
with st.sidebar:
    st.markdown(f'<div class="user-welcome">Welcome, {user["name"]}</div>', unsafe_allow_html=True)
    
    if st.button("Logout"):
        st.session_state.user_token = None
        st.rerun()

# Navigation
if user.get('role') == 'admin':
    page = st.sidebar.radio("", ['Process New Data', 'View History', 'User Management'])
else:
    page = st.sidebar.radio("", ['Process New Data', 'View History'])

if page == 'Process New Data':
   
    # First, show the title and subtitle
    st.markdown('<h1 class="standard-text">Property Address Validator</h1>', unsafe_allow_html=True)
    st.markdown('<h2 class="standard-text">Process Property Data</h2>', unsafe_allow_html=True)

    # Add the info box with properly escaped HTML
    st.markdown("""
        This tool helps you standardize and validate property addresses from your Property Shark export.

        **Step 1: Tax Class Filtering**

        The tool will first filter for only these specific tax classes:
        - CD - Residential Condominium
        - B9 - Mixed Residential & Commercial Buildings
        - B2 - Office Buildings
        - B3 - Industrial & Manufacturing
        - C0/CO - Commercial Condominium (both formats accepted)
        - B1 - Hotels & Apartments
        - C1 - Walk-up Apartments
        - A9 - Luxury Residential
        - C2 - Elevator Apartments

        *Note:* Any records with tax classes not listed above will be automatically filtered out.

        **Step 2: Address Processing**

        For all properties with valid tax classes, the tool will:
        - Professionally format and standardize addresses
        - Verify each address against official records
        - Split addresses into proper components (street, city, state, zip)
        - Remove any duplicate entries

        **Final Output**

        You'll receive:
        - A clean, verified list of properties with valid tax classes
        - Standardized and verified addresses
        - Detailed statistics about the filtering and verification process
        - A downloadable file with all processed data
    """, unsafe_allow_html=True)




    
    uploaded_file = st.file_uploader("Upload Property Shark Data Export (Excel)", type="xlsx", key="property")
    
    
    if uploaded_file:
        # Create a progress message container
        progress_message = st.empty()
        progress_message.info("Starting to process your file... 🚀")
        
        input_data = safe_read_excel(uploaded_file)
        if input_data is None or input_data.empty:
            st.error("Unable to process the uploaded file. Please ensure it contains valid data. 📋")
        else:
            try:
                file_timestamp = get_file_timestamp()
                readable_timestamp = get_readable_timestamp()
                original_filename = uploaded_file.name
                
                base_name = os.path.splitext(original_filename)[0]
                processed_filename = f"{base_name}_processed_{file_timestamp}.csv"
                
                # Create a status container to show real-time updates
                status_container = st.empty()
                
                # Define the callback function
                def status_callback(message):
                    status_container.markdown(f"""
                        <div style="color: #FFFFFF;">
                            {message}
                        </div>
                    """, unsafe_allow_html=True)
                
                # Process the data with status updates
                processed_data = data_processor.process_file(uploaded_file, status_callback)
                
                if processed_data is None or processed_data.empty:
                    progress_message.error("No valid data was found after processing. Please check your input file. 🚫")
                else:
                    # Calculate statistics
                    processed_records = len(processed_data)
                    total_records = len(input_data)
                    removed_records = total_records - processed_records
                    
                    # Update final status
                    status_container.markdown(f"""
                        <div style="color: #FFFFFF;">
                            ✅ Processing complete!<br>
                            📊 Original records: {total_records}<br>
                            🎯 Valid addresses: {processed_records}<br>
                            🗑️ Removed duplicates/invalid: {removed_records}<br>
                        </div>
                    """, unsafe_allow_html=True)
                    
# Replace the section after saving the processed data and before the download button
                    # Save the processed data
                    os.makedirs("data/outputs", exist_ok=True)
                    output_path = os.path.join("data/outputs", processed_filename)
                    processed_data.to_csv(output_path, index=False)
                    
                    # Clear the progress message
                    progress_message.success("✨ Processing complete! Your addresses have been verified and standardized.")
                    
                    # Show the results
                    st.markdown('<div class="standard-text">✅ Verified Address List</div>', unsafe_allow_html=True)
                    st.markdown('<div class="standard-text-dark">', unsafe_allow_html=True)
                    
                    # Add filter options
                    col1, col2 = st.columns(2)
                    with col1:
                        search_term = st.text_input("🔍 Search addresses", "")
                    with col2:
                        property_class = st.selectbox(
                            "📋 Filter by Property Class",
                            options=["All"] + list(data_processor.property_class_descriptions.keys()),
                            format_func=lambda x: f"{x} - {data_processor.property_class_descriptions.get(x, '')}" if x != "All" else "All Classes"
                        )
                    
                    # Filter the dataframe based on search term and property class
                    filtered_df = processed_data.copy()
                    if search_term:
                        filtered_df = filtered_df[filtered_df["Full Address"].str.contains(search_term, case=False, na=False)]
                    if property_class != "All":
                        filtered_df = filtered_df[filtered_df["Property class"] == property_class]
                    
                    # Show property class distribution
                    st.markdown("""
                        <div style="background-color: rgba(255, 255, 255, 0.1); padding: 15px; border-radius: 5px; margin: 10px 0;">
                            <h4 style="color: #FFFFFF; margin-bottom: 10px;">Property Class Distribution</h4>
                    """, unsafe_allow_html=True)
                    
                    # Create property class summary
                    class_summary = processed_data["Property class"].value_counts()
                    total_records = len(processed_data)
                    
                    for cls, count in class_summary.items():
                        percentage = (count / total_records) * 100
                        description = data_processor.property_class_descriptions.get(cls, "Unknown")
                        st.markdown(f"""
                            <div style="color: #FFFFFF; margin-bottom: 5px;">
                                {cls} - {description}: {count} records ({percentage:.1f}%)
                            </div>
                        """, unsafe_allow_html=True)
                    
                    st.markdown("</div>", unsafe_allow_html=True)
                    
                    # Display the dataframe with a note about sorting
                    st.markdown("""
                        <div style="color: #FFFFFF; font-size: 0.8em; margin-bottom: 10px;">
                            💡 Tip: Click on any column header to sort the data
                        </div>
                    """, unsafe_allow_html=True)
                    


                    # Show the dataframe with all relevant columns
                    display_columns = [
                        "Full Address",
                        "Address",
                        "City",
                        "State",
                        "Zipcode",
                        "Property class",
                        "Property Class Description",
                        "Processed Date"
                    ]

                    st.dataframe(
                        filtered_df[display_columns],
                        height=400
                    )

                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Add download button with clear instructions
                    st.markdown("""
                        <div style="background-color: rgba(255, 255, 255, 0.1); padding: 15px; border-radius: 5px; margin-top: 20px;">
                            <p style="color: #FFFFFF; margin-bottom: 10px;">
                                ⬇️ Your processed file is ready for download! 
                                Click the button below to save it to your computer.
                            </p>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    st.download_button(
                        label="📥 Download Processed Address List",
                        data=processed_data.to_csv(index=False),
                        file_name=processed_filename,
                        mime="text/csv",
                        help="Click to download your processed and verified address list"
                    )
                    
            except Exception as e:
                progress_message.error(f"Oops! Something went wrong while processing your file. Please try again or contact support if the problem persists. ❌")
                st.exception(e)

elif page == 'View History':
    st.markdown('<h1 class="standard-text">Processing History</h1>', unsafe_allow_html=True)
    
    output_dir = "data/outputs"
    if os.path.exists(output_dir):
        processed_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]
        
        if not processed_files:
            st.info("No processing history available yet.")
        else:
            processed_files.sort(key=lambda x: os.path.getmtime(os.path.join(output_dir, x)), reverse=True)
            
            st.markdown('<h2 class="standard-text">Processed Files</h2>', unsafe_allow_html=True)
            for filename in processed_files:
                file_path = os.path.join(output_dir, filename)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                with st.expander(f"{filename} (Processed on {mod_time.strftime('%B %d, %Y at %I:%M %p')})"):
                    try:
                        df = pd.read_csv(file_path)
                        st.markdown('<div class="standard-text-dark">', unsafe_allow_html=True)
                        st.dataframe(df)
                        st.markdown('</div>', unsafe_allow_html=True)
                        
                        with open(file_path, 'rb') as f:
                            st.download_button(
                                label=f"Download {filename}",
                                data=f,
                                file_name=filename,
                                mime="text/csv"
                            )
                    except Exception as e:
                        st.error(f"Error loading file: {str(e)}")
    else:
        st.info("No processing history available yet.")

elif page == 'User Management' and user.get('role') == 'admin':
    st.markdown('<h1 class="standard-text">User Management</h1>', unsafe_allow_html=True)
    
    with st.form("add_user_form", clear_on_submit=True):
        st.markdown('<h2 class="standard-text">Add New User</h2>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        with col1:
            new_email = st.text_input("Email (@sothebysrealty.com)")
            new_name = st.text_input("Full Name")
        with col2:
            new_password = st.text_input("Password", type="password")
            confirm_password = st.text_input("Confirm Password", type="password")
        
        submitted = st.form_submit_button("Add User")
        
        if submitted:
            if new_password != confirm_password:
                st.error("Passwords do not match!")
            elif not new_email or not new_name or not new_password:
                st.error("All fields are required!")
            else:
                try:
                    auth_handler.add_user(new_email, new_password, new_name, user['email'])
                    st.success(f"Successfully added user: {new_email}")
                except ValueError as e:
                    st.error(str(e))
                except Exception as e:
                    st.error(f"An error occurred: {str(e)}")
    
    # Display current users
    st.markdown('<h2 class="standard-text">Current Users</h2>', unsafe_allow_html=True)
    users = auth_handler.get_all_users()
    
    if users:
        # Prepare user data for display
        user_data = []
        for email, info in users.items():
            user_data.append({
                'Email': email,
                'Name': info['name'],
                'Role': info['role'].capitalize(),
                'Created On': info.get('created_at', 'N/A'),
                'Created By': info.get('created_by', 'N/A')
            })
        
        # Display user table
        st.markdown('<div class="standard-text-dark">', unsafe_allow_html=True)
        user_df = pd.DataFrame(user_data)
        st.dataframe(user_df, hide_index=True)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Delete user section
        st.markdown('<h2 class="standard-text">Delete User</h2>', unsafe_allow_html=True)
        
        # Create selectbox for user deletion
        delete_email = st.selectbox(
            "Select user to delete",
            options=[email for email in users.keys() if email != user['email']],
            format_func=lambda x: f"{x} ({users[x]['name']})"
        )
        
        # Delete user button and functionality
        if st.button("Delete Selected User", type="secondary"):
            if delete_email:
                try:
                    auth_handler.delete_user(delete_email, user['email'])
                    st.success(f"Successfully deleted user: {delete_email}")
                    st.rerun()
                except Exception as e:
                    st.error(f"Error deleting user: {str(e)}")
    else:
        st.info("No users found in the system.")

================
File: data/cache/address_cache.json
================
{"79ef6b8db258c1d4ff0625061ed3cfbc": {"full_address": "126, Fort Greene Place, Brooklyn, Kings County, New York, 11217, United States", "components": {"Address": "126 Fort Greene Place", "City": "New York", "State": "New York", "Zipcode": "11217"}}, "01243c7fa1e1996947a04f57ef51350b": {"full_address": "68, Adelphi Street, Brooklyn, Kings County, New York, 11205, United States", "components": {"Address": "68 Adelphi Street", "City": "New York", "State": "New York", "Zipcode": "11205"}}, "aa07dc85049e52c30b188e7de8bf07b7": {"full_address": "199, DeKalb Avenue, Clinton Hill, Fort Greene, Kings County, New York, 11205, United States", "components": {"Address": "199 DeKalb Avenue", "City": "New York", "State": "New York", "Zipcode": "11205"}}, "aec0ddb447c213e00ce111b135cc6789": {"full_address": "288, Carlton Avenue, Brooklyn, Kings County, New York, 11205, United States", "components": {"Address": "288 Carlton Avenue", "City": "New York", "State": "New York", "Zipcode": "11205"}}, "1a0a31589300f36b73bd733f16fb0f13": {"full_address": "45, South Elliott Place, Brooklyn, Kings County, New York, 11217, United States", "components": {"Address": "45 South Elliott Place", "City": "New York", "State": "New York", "Zipcode": "11217"}}, "cc852864eb42abbc87f92663d7c43d6a": {"full_address": "163, Adelphi Street, Brooklyn, Kings County, New York, 11205, United States", "components": {"Address": "163 Adelphi Street", "City": "New York", "State": "New York", "Zipcode": "11205"}}, "fc425b7a98862871d7735066c1d4f7d2": {"full_address": "56, South Oxford Street, Brooklyn, Kings County, New York, 11217, United States", "components": {"Address": "56 South Oxford Street", "City": "New York", "State": "New York", "Zipcode": "11217"}}}

================
File: dir.txt
================
real_estate_validator/
├── app.py
├── requirements.txt
├── vercel.json
├── .env.example
├── .gitignore
├── README.md
├── assets/
│   ├── favicon.ico
│   └── sothebys-logo.png
├── .streamlit/
│   └── config.toml
└── utils/
    ├── __init__.py
    ├── address_standardizer.py
    ├── auth.py
    ├── data_processor.py
    └── database.py

================
File: README.md
================
# SIR Prospect Address Validator

## TLDR - Critical Information
- **Live App URL**: [Streamlit-URL]
- **Admin Login**: 
  - Email: ****.*****@********.******
  - Password: **********
- **Key Features**: Upload Property Shark exports, standardize addresses, remove duplicates
- **Access**: Only @********.****** email domains
- **Support**: REDACTED
- **Security**: JWT authentication, role-based access, domain-restricted
- **Data**: All processed files stored in MongoDB with backup
- **Deployment**: Hosted on Vercel, MongoDB Atlas backend

## Quick Start
1. Login with Sotheby's email
2. Upload Property Shark Excel export
3. Get standardized address list
4. Download processed file
5. View processing history

A secure, enterprise-grade Streamlit application for processing and validating real estate property addresses, specifically designed for SIR.

## Features

### Core Functionality
- Upload and process Property Shark data exports
- Standardize property addresses
- Remove duplicate entries
- Track processing history
- View and download processed files

### Enterprise Features
- Secure authentication system
- Domain-restricted access (@********.******)
- User management interface
- Processing history tracking
- Role-based access control
- Professional branding


## Authentication

### Initial Admin Access
- Email: REDACTED
- Password: Contact administrator for credentials

### User Management
- Only administrators can add new users
- Domain restricted to @********.******
- Secure password policies enforced
- User activity tracking

## Usage

1. Login with Sotheby's credentials
2. Navigate using the sidebar menu:
   - Process New Data
   - View History
   - User Management (Admin only)

### Processing Data
1. Upload Property Shark Excel export
2. System will automatically:
   - Standardize addresses
   - Remove duplicates
   - Generate downloadable processed file

### Viewing History
- Access all previously processed files
- Download original or processed files
- View processing statistics

### User Management (Admin)
- Add new users
- View user list
- Manage user access
- Track user activity

## Development

### Technology Stack
- Python 3.9+
- Streamlit for web interface
- Pandas for data processing
- MongoDB for data storage
- JWT for authentication
- bcrypt for password hashing

### Styling
- Custom Sotheby's branding
- Professional UI/UX
- Responsive design
- Consistent color scheme (#002A5C)

## Security Features
- Domain-restricted access
- Secure password handling
- JWT token authentication
- Session management
- Role-based access control
- Secure file handling

## Deployment

The application is deployed on Vercel and can be accessed at:
[Your-Streamlit-URL]

### Production Configuration
1. Set up MongoDB Atlas cluster
2. Configure Vercel environment variables
3. Add Vercel's IP addresses to MongoDB network access

## Support

For support, please contact:
- Technical Issues: [Your Technical Contact]
- User Access: REDACTED

## License

Internal use only - SIR

## Setting up Nominatim User Agent

To use the Nominatim service for address normalization, you need to set up a user agent. This can be done by adding the following line to your environment variables:

```bash
NOMINATIM_USER_AGENT=your_unique_user_agent
```

Replace `your_unique_user_agent` with a unique identifier for your application. This is required by Nominatim to identify your requests.

================
File: requirements.txt
================
streamlit==1.29.0
pandas==2.1.3
openpyxl==3.1.2
numpy==1.26.2
pymongo==4.6.1
dnspython==2.4.2
python-dotenv==1.0.0
bcrypt==4.1.1
python-jose==3.3.0
PyJWT==2.8.0
watchdog==3.0.0
python-multipart==0.0.6
typing-extensions==4.7.1
geopy==2.2.0

================
File: setup_mongodb.py
================
"""
MongoDB setup and test script for Sotheby's Address Validator
"""

import os
from dotenv import load_dotenv
from pymongo import MongoClient, ASCENDING, DESCENDING
import gridfs
from datetime import datetime

# Load environment variables
load_dotenv()

def setup_mongodb():
    """Set up MongoDB connection and initialize collections"""
    try:
        # Get MongoDB URI from environment variables
        uri = os.getenv('MONGODB_URI')
        if not uri:
            raise ValueError("MONGODB_URI not found in environment variables")

        # Create MongoDB client
        client = MongoClient(uri)
        
        # Test connection with ping
        client.admin.command('ping')
        print("✅ Successfully connected to MongoDB!")

        # Get database
        db = client.sothebys_validator
        
        # Initialize GridFS
        fs = gridfs.GridFS(db)
        
        # Set up collections with indexes
        setup_collections(db)
        
        return client, db, fs
    
    except Exception as e:
        print(f"❌ Error setting up MongoDB: {str(e)}")
        return None, None, None

def setup_collections(db):
    """Set up collections and their indexes"""
    try:
        # Users collection
        if 'users' not in db.list_collection_names():
            users = db.create_collection('users')
            users.create_index([('email', ASCENDING)], unique=True)
            print("✅ Created users collection with email index")

        # Processing logs collection
        if 'processing_logs' not in db.list_collection_names():
            logs = db.create_collection('processing_logs')
            logs.create_index([('timestamp', DESCENDING)])
            logs.create_index([('user_email', ASCENDING)])
            print("✅ Created processing_logs collection with indexes")

        # Test inserting a processing log
        test_log = {
            'filename': 'test.xlsx',
            'user_email': 'test@sothebysrealty.com',
            'timestamp': datetime.now(),
            'status': 'success',
            'records_processed': 10,
            'records_filtered': 5
        }
        db.processing_logs.insert_one(test_log)
        print("✅ Successfully inserted test processing log")

        # Test GridFS
        fs = gridfs.GridFS(db)
        test_file_id = fs.put(
            b"Test data",
            filename="test.txt",
            metadata={'test': True}
        )
        fs.delete(test_file_id)
        print("✅ Successfully tested GridFS operations")

    except Exception as e:
        print(f"❌ Error setting up collections: {str(e)}")

def display_database_info(db):
    """Display information about the database and its collections"""
    try:
        print("\n📊 Database Information:")
        print("-" * 50)
        
        # List collections
        collections = db.list_collection_names()
        print(f"\nCollections in database:")
        for collection in collections:
            count = db[collection].count_documents({})
            print(f"- {collection}: {count} documents")

        # Display indexes for each collection
        print("\nIndexes:")
        for collection in collections:
            print(f"\n{collection} indexes:")
            indexes = db[collection].list_indexes()
            for index in indexes:
                print(f"- {index['name']}: {index['key']}")

    except Exception as e:
        print(f"❌ Error displaying database info: {str(e)}")

def main():
    """Main setup function"""
    print("\n🚀 Setting up MongoDB for Sotheby's Address Validator...\n")
    
    # Set up MongoDB connection and collections
    client, db, fs = setup_mongodb()
    
    # Check if setup was successful
    if client is not None and db is not None and fs is not None:
        # Display database information
        display_database_info(db)
        
        print("\n✨ MongoDB setup complete!")
        
        # Clean up
        client.close()
    else:
        print("\n❌ MongoDB setup failed")

if __name__ == "__main__":
    main()

================
File: static/styles.css
================
/* Main app styling and background */
.stApp {
    background-color: #002A5C;
    color: #FFFFFF;
    max-width: 1200px;
    margin: 0 auto;
}

.stApp > header {
    background-color: transparent;
}

/* Logo containers */
.logo-container-login, .logo-container-main {
    text-align: center;
    padding: 1rem 0;
    margin-bottom: 2rem;
    border-radius: 8px;
}

.logo-container-login {
    background-color: transparent;
}

.logo-container-main {
    background-color: #002A5C;
}

.logo-container-login img,
.logo-container-main img {
    max-width: 200px;
}

/* Typography */
h1, h2, h3, h4, h5, h6 {
    color: #FFFFFF !important;
    font-weight: 600 !important;
}

p, span, div {
    color: inherit;
}

/* Login specific styling */
.login-container {
    max-width: 400px;
    margin: 0 auto;
    padding: 2rem;
    background-color: #FFFFFF;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    color: #002A5C !important;
}

/* Login message styling */
.login-container h3,
#use-your-sothebys-realty-email-to-login {
    font-size: 0.9rem;
    margin: 0 0 1.5rem 0;
    color: #002A5C !important;
    text-align: center;
    font-weight: 500 !important;
    background: none !important;
    border: none !important;
    box-shadow: none !important;
    padding: 0 !important;
}

/* Remove box around login message */
div[data-testid="stForm"] > div:first-child {
    border: none !important;
    box-shadow: none !important;
    padding: 0 !important;
    margin: 0 !important;
}

.login-container .stTextInput > label {
    color: #002A5C !important;
}

/* Form elements */
.stTextInput>div>div>input {
    border: 1px solid #002A5C !important;
    color: #002A5C !important;
    background-color: #FFFFFF;
    border-radius: 4px;
}

.stTextInput>div>div>input:focus {
    border: 2px solid #002A5C !important;
    box-shadow: none !important;
}

.stTextInput > label,
.stSelectbox > label,
.stFileUploader > label {
    color: #FFFFFF !important;
}

/* Buttons */
.stButton>button {
    background-color: #002A5C !important;
    color: #FFFFFF !important;
    border: 1px solid #FFFFFF !important;
    border-radius: 4px !important;
    padding: 0.5rem 1rem !important;
    font-weight: 500 !important;
    width: 100% !important;
    margin-top: 1rem !important;
}

.stButton>button:hover {
    background-color: #003A7C !important;
}

================
File: utils/__init__.py
================
from .address_standardizer import AddressStandardizer
from .data_processor import DataProcessor
from .auth import AuthHandler

================
File: utils/address_standardizer.py
================
# utils/address_standardizer.py
import os
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError, GeocoderQuotaExceeded
from geopy.extra.rate_limiter import RateLimiter
import logging
from dotenv import load_dotenv
import time
import re
from functools import lru_cache
import concurrent.futures
import threading
import random
import json
import hashlib

class AddressStandardizer:
    """Optimized address standardizer with caching and rate limiting."""
    
    def __init__(self):
        load_dotenv()
        
        # Configure logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Get user agent from environment variables
        self.user_agent = os.getenv('NOMINATIM_USER_AGENT')
        if not self.user_agent:
            raise ValueError("NOMINATIM_USER_AGENT not found in environment variables")
        
        # Initialize thread-local storage for geocoders
        self._thread_local = threading.local()
        
        # Cache for standardized addresses
        self._address_cache = {}
        self._load_cache()
        
        # Rate limiting settings
        self.min_delay = 1.1  # Minimum delay between requests
        self.max_delay = 2.0  # Maximum delay for randomization
        self.max_retries = 5  # Maximum number of retries
        self.error_wait = 5.0  # Wait time after error
        self.batch_size = 10   # Process in smaller batches
        
        # Initialize rate limiter with more conservative settings
        self._init_rate_limiter()

    def _init_rate_limiter(self):
        """Initialize rate limiter with conservative settings."""
        self.geocode = RateLimiter(
            self.geolocator.geocode,
            min_delay_seconds=self.min_delay,
            max_retries=self.max_retries,
            error_wait_seconds=self.error_wait,
            swallow_exceptions=False
        )

    @property
    def geolocator(self):
        """Thread-safe geolocator instance with rotating user agents."""
        if not hasattr(self._thread_local, 'geolocator'):
            # Create unique user agent for this thread
            thread_id = threading.get_ident()
            user_agent = f"{self.user_agent}_{thread_id}_{random.randint(1000, 9999)}"
            
            self._thread_local.geolocator = Nominatim(
                user_agent=user_agent,
                timeout=10
            )
        return self._thread_local.geolocator

    def _get_cache_file(self):
        """Get cache file path."""
        cache_dir = "data/cache"
        os.makedirs(cache_dir, exist_ok=True)
        return os.path.join(cache_dir, "address_cache.json")

    def _load_cache(self):
        """Load address cache from file."""
        try:
            cache_file = self._get_cache_file()
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    self._address_cache = json.load(f)
        except Exception as e:
            self.logger.error(f"Error loading cache: {str(e)}")
            self._address_cache = {}

    def _save_cache(self):
        """Save address cache to file."""
        try:
            cache_file = self._get_cache_file()
            with open(cache_file, 'w') as f:
                json.dump(self._address_cache, f)
        except Exception as e:
            self.logger.error(f"Error saving cache: {str(e)}")

    def _get_cache_key(self, address):
        """Generate cache key for address."""
        if not address:
            return None
        return hashlib.md5(address.lower().encode()).hexdigest()

    @lru_cache(maxsize=1000)
    def _clean_address(self, address):
        """Clean address string."""
        if not address:
            return address
        
        address = str(address)
        address = re.sub(r'\s+', ' ', address.strip())
        address = re.sub(r'[^a-zA-Z0-9\s,\.#-]', '', address)
        
        return address

    def parse_normalized_address(self, full_address):
        """Parse address into components with fallback."""
        try:
            if not full_address:
                return None
            
            # Check cache first
            cache_key = self._get_cache_key(full_address)
            if cache_key in self._address_cache:
                return self._address_cache[cache_key].get('components')
            
            # Add random delay to help with rate limiting
            time.sleep(random.uniform(self.min_delay, self.max_delay))
            
            # Try geocoding with retries
            for attempt in range(self.max_retries):
                try:
                    location = self.geolocator.geocode(
                        full_address,
                        addressdetails=True,
                        language='en',
                        exactly_one=True
                    )
                    
                    if location and location.raw.get('address'):
                        components = self._extract_components(location.raw['address'])
                        
                        # Cache the result
                        self._address_cache[cache_key] = {
                            'full_address': location.address,
                            'components': components
                        }
                        self._save_cache()
                        
                        return components
                    
                except (GeocoderTimedOut, GeocoderQuotaExceeded):
                    if attempt < self.max_retries - 1:
                        time.sleep(self.error_wait * (attempt + 1))
                    continue
                    
                except Exception as e:
                    self.logger.error(f"Error in geocoding: {str(e)}")
                    break
            
            # Fallback: Parse address manually
            components = self._manual_parse(full_address)
            if components:
                self._address_cache[cache_key] = {
                    'full_address': full_address,
                    'components': components
                }
                self._save_cache()
            
            return components
            
        except Exception as e:
            self.logger.error(f"Error parsing address: {str(e)}")
            return None

    def _extract_components(self, address_parts):
        """Extract address components from geocoded result."""
        # Extract house number and street
        house_number = address_parts.get('house_number', '')
        street = address_parts.get('road', '') or address_parts.get('street', '')
        
        if house_number and street:
            street_address = f"{house_number} {street}"
        else:
            street_address = street or house_number
        
        # Get city (with fallbacks)
        city = (address_parts.get('city') or 
               address_parts.get('town') or 
               address_parts.get('village') or 
               address_parts.get('suburb') or
               address_parts.get('neighbourhood'))
        
        # Get state and postal code
        state = address_parts.get('state', '')
        postal_code = address_parts.get('postcode', '')
        
        return {
            'Address': street_address.strip(),
            'City': city.strip() if city else '',
            'State': state.strip() if state else '',
            'Zipcode': postal_code.strip() if postal_code else ''
        }

    def _manual_parse(self, full_address):
        """Manual address parsing as fallback."""
        try:
            parts = full_address.split(',')
            if len(parts) >= 3:
                # Get state and zip from last part
                state_zip = parts[-1].strip().split()
                state = state_zip[0] if len(state_zip) > 0 else ''
                zipcode = state_zip[-1] if len(state_zip) > 1 else ''
                
                # Get city from second to last part
                city = parts[-2].strip()
                
                # Get street address from remaining parts
                address = ','.join(parts[:-2]).strip()
                
                return {
                    'Address': address,
                    'City': city,
                    'State': state,
                    'Zipcode': zipcode
                }
            
            return None
            
        except Exception:
            return None

    def standardize_batch(self, addresses, max_workers=3):
        """
        Standardize addresses in batches with rate limiting.
        """
        results = {}
        total_addresses = len(addresses)
        
        # Process in smaller batches
        for i in range(0, total_addresses, self.batch_size):
            batch = addresses[i:i + self.batch_size]
            
            for address in batch:
                try:
                    # Check cache first
                    cache_key = self._get_cache_key(address)
                    if cache_key in self._address_cache:
                        cached_result = self._address_cache[cache_key]
                        results[address] = cached_result
                        continue
                    
                    # Add random delay
                    time.sleep(random.uniform(self.min_delay, self.max_delay))
                    
                    components = self.parse_normalized_address(address)
                    if components:
                        results[address] = {
                            'full_address': address,
                            'components': components
                        }
                    else:
                        results[address] = {
                            'full_address': address,
                            'components': None
                        }
                        
                except Exception as e:
                    self.logger.error(f"Error processing address {address}: {str(e)}")
                    results[address] = {
                        'full_address': address,
                        'components': None
                    }
            
            # Small delay between batches
            time.sleep(random.uniform(self.min_delay, self.max_delay))
        
        return results

    def standardize(self, address):
        """
        Standardize single address with caching.
        """
        if not address:
            return {'full_address': address, 'components': None}
        
        try:
            cache_key = self._get_cache_key(address)
            if cache_key in self._address_cache:
                return self._address_cache[cache_key]
            
            components = self.parse_normalized_address(address)
            result = {
                'full_address': address,
                'components': components
            }
            
            self._address_cache[cache_key] = result
            self._save_cache()
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error standardizing address {address}: {str(e)}")
            return {
                'full_address': address,
                'components': None
            }

================
File: utils/auth.py
================
# utils/auth.py
import os
from dotenv import load_dotenv
import bcrypt
import jwt
from datetime import datetime, timedelta
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AuthHandler:
    def __init__(self):
        load_dotenv()
        self.secret_key = os.getenv('JWT_SECRET_KEY')
        if not self.secret_key:
            raise ValueError("JWT_SECRET_KEY not found in environment variables")
            
        # Initialize with default admin user (case insensitive key)
        admin_email = 'Matt.Sadik@sothebys.realty'
        admin_password = "Chance72$$"
        salt = bcrypt.gensalt()
        hashed_password = bcrypt.hashpw(admin_password.encode('utf-8'), salt)
        
        self.users = {
            admin_email: {
                'password': hashed_password,
                'name': 'Matt Sadik',
                'role': 'admin',
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'created_by': 'system'
            }
        }
        logger.info("AuthHandler initialized with admin user")

    def verify_email_domain(self, email):
        """Verify email belongs to Sotheby's domain."""
        email = email.lower()
        valid = email.endswith(('sothebys.realty', 'sothebysrealty.com'))
        logger.info(f"Email domain verification for {email}: {'valid' if valid else 'invalid'}")
        return valid

    def add_user(self, email, password, name, added_by):
        """Add a new user to the system."""
        try:
            if not self.verify_email_domain(email):
                raise ValueError("Only Sotheby's email addresses are allowed")
            
            # Case-insensitive check for existing user
            if any(existing.lower() == email.lower() for existing in self.users.keys()):
                raise ValueError("User already exists")
            
            # Hash password
            salt = bcrypt.gensalt()
            hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
            
            self.users[email] = {
                'password': hashed,
                'name': name,
                'role': 'user',
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'created_by': added_by
            }
            logger.info(f"Successfully added new user: {email}")
            return True
            
        except Exception as e:
            logger.error(f"Error adding user {email}: {str(e)}")
            raise

    def is_admin(self, email):
        """Check if user is an admin."""
        # Case-insensitive check for admin role
        matching_email = next(
            (stored_email for stored_email in self.users.keys() 
             if stored_email.lower() == email.lower()),
            None
        )
        is_admin = matching_email and self.users[matching_email].get('role') == 'admin'
        logger.info(f"Admin check for {email}: {is_admin}")
        return is_admin

    def get_all_users(self):
        """Get list of all users (excluding passwords)."""
        try:
            users = {
                email: {
                    'name': info['name'],
                    'role': info.get('role', 'user'),
                    'created_at': info.get('created_at', 'N/A'),
                    'created_by': info.get('created_by', 'N/A')
                }
                for email, info in self.users.items()
            }
            logger.info(f"Retrieved {len(users)} users")
            return users
        except Exception as e:
            logger.error(f"Error getting users: {str(e)}")
            return {}

    def login(self, email, password):
        """Authenticate user and return JWT token."""
        try:
            # Case-insensitive email check
            matching_email = next(
                (stored_email for stored_email in self.users.keys() 
                 if stored_email.lower() == email.lower()),
                None
            )
            
            if not matching_email:
                logger.warning(f"Login failed: User {email} not found")
                return None
            
            stored_password = self.users[matching_email]['password']
            if isinstance(stored_password, str):
                stored_password = stored_password.encode('utf-8')
            
            if bcrypt.checkpw(password.encode('utf-8'), stored_password):
                token = jwt.encode({
                    'email': matching_email,  # Use the correctly-cased email
                    'name': self.users[matching_email]['name'],
                    'role': self.users[matching_email].get('role', 'user'),
                    'exp': datetime.utcnow() + timedelta(days=1)
                }, self.secret_key, algorithm='HS256')
                
                logger.info(f"Login successful for user: {matching_email}")
                return token
            
            logger.warning(f"Login failed: Invalid password for {email}")
            return None
            
        except Exception as e:
            logger.error(f"Login error for {email}: {str(e)}")
            return None

    def verify_token(self, token):
        """Verify JWT token and return user info."""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=['HS256'])
            # Case-insensitive email check
            matching_email = next(
                (stored_email for stored_email in self.users.keys() 
                 if stored_email.lower() == payload['email'].lower()),
                None
            )
            
            if matching_email:
                logger.info(f"Token verified for user: {matching_email}")
                return {
                    'email': matching_email,
                    'name': self.users[matching_email]['name'],
                    'role': self.users[matching_email].get('role', 'user')
                }
            
            logger.warning("Token verification failed: User not found")
            return None
            
        except jwt.ExpiredSignatureError:
            logger.warning("Token verification failed: Token expired")
            return None
        except jwt.InvalidTokenError:
            logger.warning("Token verification failed: Invalid token")
            return None
        except Exception as e:
            logger.error(f"Token verification error: {str(e)}")
            return None

    def delete_user(self, email, admin_email):
        """Delete a user (admin only)."""
        try:
            if not self.is_admin(admin_email):
                raise ValueError("Only administrators can delete users")
                
            # Case-insensitive checks
            admin_match = next(
                (stored_email for stored_email in self.users.keys() 
                 if stored_email.lower() == admin_email.lower()),
                None
            )
            user_match = next(
                (stored_email for stored_email in self.users.keys() 
                 if stored_email.lower() == email.lower()),
                None
            )
            
            if email.lower() == admin_email.lower():
                raise ValueError("Cannot delete your own admin account")
            if not user_match:
                raise ValueError("User not found")
            
            del self.users[user_match]
            logger.info(f"User {email} successfully deleted by admin {admin_email}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting user {email}: {str(e)}")
            raise

================
File: utils/data_processor.py
================
# utils/data_processor.py
import pandas as pd
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from utils.address_standardizer import AddressStandardizer

class DataProcessor:
    def __init__(self, valid_property_classes):
        """
        Initialize the DataProcessor with valid property classes.
        
        Property Class Codes:
        - CD: Residential Condominium
        - B9: Mixed Residential & Commercial Buildings
        - B2: Office Buildings
        - B3: Industrial & Manufacturing
        - C0/CO: Commercial Condominium (both formats accepted)
        - B1: Hotels & Apartments
        - C1: Walk-up Apartments
        - A9: Luxury / High-End Residential
        - C2: Elevator Apartments
        """
        self.valid_property_classes = valid_property_classes
        self.address_standardizer = AddressStandardizer()
        self.logger = logging.getLogger(__name__)
        
        # Property class descriptions for user feedback
        self.property_class_descriptions = {
            "CD": "Residential Condominium",
            "B9": "Mixed Residential & Commercial",
            "B2": "Office Buildings",
            "B3": "Industrial & Manufacturing",
            "C0": "Commercial Condominium",
            "CO": "Commercial Condominium",  # Alternative format
            "B1": "Hotels & Apartments",
            "C1": "Walk-up Apartments",
            "A9": "Luxury Residential",
            "C2": "Elevator Apartments"
        }
        
        # Mapping for standardizing property class formats
        self.property_class_standardization = {
            "CO": "C0",  # Map CO to C0 format
            "C0": "C0",  # Keep C0 as is
        }

    def standardize_property_class(self, property_class):
        """Standardize property class format."""
        if not property_class:
            return property_class
        return self.property_class_standardization.get(property_class, property_class)

    def load_data(self, file_path):
        """Load data with optimized settings."""
        try:
            return pd.read_excel(
                file_path,
                engine='openpyxl',
                dtype={
                    'Zipcode': str,
                    'Address': str,
                    'City': str,
                    'State': str,
                    'Property class': str
                }
            )
        except Exception as e:
            self.logger.error(f"Error loading data: {str(e)}")
            raise

    def analyze_property_classes(self, df):
        """
        Analyze property classes in the dataset and provide detailed feedback.
        
        Returns:
            tuple: (filtered_df, stats_dict) where stats_dict contains analysis results
        """
        try:
            # First standardize the property classes
            df = df.copy()
            df["Property class"] = df["Property class"].apply(self.standardize_property_class)
            
            # Count occurrences of each property class
            class_counts = df["Property class"].value_counts()
            
            # Separate valid and invalid classes
            valid_classes = {cls: count for cls, count in class_counts.items() 
                            if cls in self.valid_property_classes}
            invalid_classes = {cls: count for cls, count in class_counts.items() 
                             if cls not in self.valid_property_classes}
            
            # Filter the dataframe
            filtered_df = df[df["Property class"].isin(self.valid_property_classes)]
            
            # Calculate total CO/C0 records
            co_count = len(df[df["Property class"].isin(["CO", "C0"])])
            
            # Create statistics dictionary
            stats = {
                "total_records": len(df),
                "valid_records": len(filtered_df),
                "filtered_out": len(df) - len(filtered_df),
                "valid_classes": {
                    cls: {
                        "count": count,
                        "description": self.property_class_descriptions.get(cls, "Unknown"),
                        "percentage": (count / len(df) * 100) if len(df) > 0 else 0
                    }
                    for cls, count in valid_classes.items()
                },
                "invalid_classes": {
                    cls: count for cls, count in invalid_classes.items()
                },
                "co_records": co_count
            }
            
            return filtered_df, stats
            
        except Exception as e:
            self.logger.error(f"Error analyzing property classes: {str(e)}")
            raise

    def filter_data(self, df, status_callback=None):
        """Enhanced data filtering with detailed statistics."""
        try:
            if status_callback:
                status_callback("Analyzing property classes...")
            
            filtered_df, stats = self.analyze_property_classes(df)
            
            if status_callback:
                # Create detailed status message
                message = f"""
                📊 Property Class Analysis:
                Total Records: {stats['total_records']}
                Valid Records: {stats['valid_records']} ({(stats['valid_records']/stats['total_records']*100):.1f}%)
                Filtered Out: {stats['filtered_out']} ({(stats['filtered_out']/stats['total_records']*100):.1f}%)
                
                ℹ️ Note: Both 'C0' and 'CO' are treated as Commercial Condominium class
                
                ✅ Valid Property Classes:
                """
                
                # Add details for each valid class
                for cls, info in stats['valid_classes'].items():
                    description = info['description']
                    count = info['count']
                    percentage = info['percentage']
                    
                    # Special handling for C0/CO display
                    if cls in ["C0", "CO"]:
                        message += f"\n• Commercial Condominium (C0/CO): {count} ({percentage:.1f}%)"
                    else:
                        message += f"\n• {cls} - {description}: {count} ({percentage:.1f}%)"
                
                if stats['invalid_classes']:
                    message += "\n\n❌ Filtered Out Classes:"
                    for cls, count in stats['invalid_classes'].items():
                        message += f"\n• {cls}: {count}"
                
                status_callback(message)
            
            # Drop unnecessary columns
            filtered_df = filtered_df.drop(columns=["Block & Lot"], errors="ignore")
            
            return filtered_df, stats
            
        except Exception as e:
            self.logger.error(f"Error in filter_data: {str(e)}")
            raise

    def create_full_addresses(self, df):
        """Efficiently create full addresses for all rows."""
        try:
            # Ensure all address components are strings and handle NaN
            components = ['Address', 'City', 'State', 'Zipcode']
            for col in components:
                if col in df.columns:
                    df[col] = df[col].fillna('').astype(str).str.strip()

            # Vectorized operations for combining address components
            full_addresses = (
                df['Address'].str.strip() + ', ' +
                df['City'].str.strip() + ', ' +
                df['State'].str.strip() + ' ' +
                df['Zipcode'].str.strip()
            )
            
            return full_addresses.str.strip(', ')
            
        except Exception as e:
            self.logger.error(f"Error creating full addresses: {str(e)}")
            raise

    def standardize_addresses(self, df, status_callback=None):
        """
        Batch process address standardization with progress updates and component handling.
        """
        try:
            if status_callback:
                status_callback("Creating full addresses...")
            
            # First create all full addresses
            df['Full Address'] = self.create_full_addresses(df)
            
            # Get unique addresses to avoid processing duplicates
            unique_addresses = df['Full Address'].unique()
            
            if status_callback:
                status_callback(f"Standardizing {len(unique_addresses)} unique addresses...")
            
            # Batch process all unique addresses
            standardized_results = self.address_standardizer.standardize_batch(unique_addresses)
            
            # Create a mapping for full addresses
            address_mapping = {
                addr: result['full_address']
                for addr, result in standardized_results.items()
            }
            
            # Update the DataFrame with standardized full addresses
            df['Full Address'] = df['Full Address'].map(address_mapping)
            
            if status_callback:
                status_callback("Updating address components...")
            
            # Update individual components
            for idx, row in df.iterrows():
                original_address = row['Full Address']
                if original_address in standardized_results:
                    result = standardized_results[original_address]
                    if result.get('components'):
                        components = result['components']
                        for field, value in components.items():
                            if field in df.columns:
                                df.at[idx, field] = value
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error in standardize_addresses: {str(e)}")
            return df

    def remove_duplicates(self, df):
        """Optimized duplicate removal."""
        try:
            # Convert sale date once for all rows
            if 'Sale date' in df.columns:
                df['Sale date'] = pd.to_datetime(df['Sale date'], errors='coerce')
                df = df.sort_values('Sale date', ascending=False)

            return df.drop_duplicates(subset=['Full Address'], keep='first').reset_index(drop=True)
            
        except Exception as e:
            self.logger.error(f"Error removing duplicates: {str(e)}")
            return df

    def process_file(self, file_path, status_callback=None):
        """
        Process file with enhanced property class filtering and status updates.
        """
        try:
            if status_callback:
                status_callback("Loading data...")
            
            df = self.load_data(file_path)
            initial_count = len(df)
            
            if status_callback:
                status_callback(f"Analyzing {initial_count} records...")
            
            filtered_df, filter_stats = self.filter_data(df, status_callback)
            filtered_count = len(filtered_df)
            
            if filtered_count == 0:
                if status_callback:
                    status_callback("❌ No valid property classes found in the data.")
                return None
            
            if status_callback:
                status_callback(f"Standardizing {filtered_count} addresses...")
            
            standardized_df = self.standardize_addresses(filtered_df, status_callback)
            
            if status_callback:
                status_callback("Removing duplicates...")
            
            deduped_df = self.remove_duplicates(standardized_df)
            final_count = len(deduped_df)
            
            # Add processing timestamp and filter statistics
            deduped_df["Processed Date"] = datetime.now().strftime("%B %d, %Y at %I:%M %p")
            
            # Add property class descriptions
            deduped_df['Property Class Description'] = deduped_df['Property class'].map(self.property_class_descriptions)
            
            # Reorder columns to put important information first
            columns = [
                'Full Address', 
                'Address',
                'City', 
                'State',
                'Zipcode',
                'Property class', 
                'Property Class Description'
            ] + [
                col for col in deduped_df.columns if col not in [
                    'Full Address', 'Address', 'City', 'State', 'Zipcode',
                    'Property class', 'Property Class Description'
                ]
            ]
            deduped_df = deduped_df[columns]
            
            if status_callback:
                status_callback(f"""
                ✅ Processing complete!
                
                📈 Final Statistics:
                • Initial records: {initial_count}
                • Valid property classes: {filtered_count}
                • Final records after deduplication: {final_count}
                • Removed duplicates: {filtered_count - final_count}
                
                Most common property types in final dataset:
                {deduped_df['Property class'].value_counts().head(3).to_string()}
                """)
            
            return deduped_df
            
        except Exception as e:
            self.logger.error(f"Error processing file: {str(e)}")
            if status_callback:
                status_callback(f"Error: {str(e)}")
            return None

================
File: utils/database.py
================
# utils/database.py
import os
from pymongo import MongoClient
from datetime import datetime
import gridfs
import pandas as pd
import io

class DatabaseHandler:
    def __init__(self):
        # Get MongoDB connection string from environment variable
        mongo_uri = os.getenv('MONGODB_URI')
        if not mongo_uri:
            raise ValueError("MongoDB URI not found in environment variables")
        
        self.client = MongoClient(mongo_uri)
        self.db = self.client.sothebys_validator
        self.fs = gridfs.GridFS(self.db)
        
    def save_file(self, filename, file_data, file_type, metadata=None):
        """Save file to GridFS with metadata."""
        return self.fs.put(
            file_data,
            filename=filename,
            file_type=file_type,
            metadata=metadata,
            upload_date=datetime.now()
        )
    
    def get_file(self, file_id):
        """Retrieve file from GridFS."""
        return self.fs.get(file_id)
    
    def save_processing_log(self, log_entry):
        """Save processing log entry to MongoDB."""
        self.db.processing_logs.insert_one({
            **log_entry,
            'timestamp': datetime.now()
        })
    
    def get_processing_logs(self):
        """Retrieve all processing logs."""
        return list(self.db.processing_logs.find().sort('timestamp', -1))
    
    def save_processed_data(self, data_df, metadata):
        """Save processed pandas DataFrame to MongoDB."""
        # Convert DataFrame to CSV string
        csv_buffer = io.StringIO()
        data_df.to_csv(csv_buffer, index=False)
        
        # Save to GridFS
        file_id = self.fs.put(
            csv_buffer.getvalue().encode('utf-8'),
            filename=f"processed_{metadata['timestamp']}.csv",
            metadata=metadata
        )
        return file_id
    
    def get_processed_data(self, file_id):
        """Retrieve processed data as DataFrame."""
        file_data = self.fs.get(file_id)
        csv_data = io.StringIO(file_data.read().decode('utf-8'))
        return pd.read_csv(csv_data)
